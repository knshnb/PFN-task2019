import os
import pickle
import numpy as np

from gnn import GraphNeuralNetwork
from parameter import MINIBATCH_SIZE, TRAIN_NUM, TEST_NUM
from optimizer import SGD, Momentum, Adam

# シャフルして、始めの{TRAIN_NUM}個を訓練に、残り{TEST_NUM}個をテストに使用
SHUFFLED_INDEX = [
    928,942,1970,1894,1528,152,594,616,836,389,267,1732,1846,234,606,640,699,950,1545,732,62,448,973,927,1980,1605,1749,1912,1700,957,0,343,1340,249,863,1489,1453,908,1728,843,185,24,1004,959,1333,460,822,74,1301,856,573,1585,242,1627,567,1805,693,1381,486,199,453,1584,679,1815,1029,1328,1813,1014,340,740,1400,1502,1268,649,1096,667,646,332,1840,983,792,220,1419,399,939,1843,1391,1819,1146,632,1559,1880,742,134,150,1206,1739,196,1774,1413,588,1599,974,382,1719,558,1793,309,1428,1271,1329,1090,1897,1944,272,584,1230,96,1141,1196,820,953,1976,1078,1744,889,11,433,1114,1716,439,1796,1431,88,1658,890,1639,934,107,1758,1926,1364,436,1411,1947,877,570,1966,1342,821,1144,1808,753,1283,1541,1025,236,1673,715,848,1421,1621,1892,421,410,1548,1779,1388,1223,3,949,374,304,1653,26,1764,1089,694,1326,55,231,1656,1906,576,273,837,1048,688,1565,420,1218,217,1807,471,1905,1608,557,1063,1118,21,1467,1020,1932,752,1464,144,794,299,1853,1198,1157,1304,206,1832,1200,1323,1498,1733,1181,929,1720,1529,22,971,177,29,1081,578,1699,669,1040,996,1080,1178,701,1515,781,902,432,566,601,1051,633,1835,548,799,867,772,592,1073,909,1082,1517,63,1059,743,868,745,1524,1150,1083,323,1121,1817,1246,327,1798,1953,225,1176,990,1718,531,83,1975,1163,240,1578,897,1873,1562,1936,1312,1252,1952,1674,832,1558,806,1648,1771,569,1664,1399,1845,375,650,854,1046,1938,1856,605,513,728,1954,413,1262,784,777,968,1755,418,1036,900,114,366,1867,339,1138,1678,1729,522,41,1844,985,915,841,1645,515,160,1782,344,1901,550,324,1522,911,723,1704,673,924,970,575,833,1396,600,1370,376,812,577,1537,1507,630,1689,839,1841,1641,737,1374,257,1997,78,1685,1034,1896,1005,1630,639,1870,882,103,266,498,228,761,1848,869,585,1613,1743,535,533,108,94,542,1574,596,313,527,43,1158,1164,1281,450,1185,472,1581,51,482,687,1572,388,492,1590,176,184,1451,147,1547,412,865,1974,1951,1611,637,838,702,1809,979,1917,1838,1914,541,1655,1165,1019,97,1441,1534,1221,1883,1949,892,1384,1454,1235,398,1148,773,1657,1886,187,456,154,710,1937,609,81,244,765,1469,1861,1094,593,1640,831,407,1241,342,543,1715,397,1825,1865,774,540,1140,511,480,1624,857,158,1346,1210,1587,189,44,1918,1533,151,876,579,461,370,1644,1752,1332,614,668,292,1278,537,1676,895,1295,1978,1557,1444,1516,437,987,1492,1750,615,547,980,210,504,476,300,1167,655,910,1746,1194,739,441,70,1806,648,1625,1098,864,1713,1430,1291,322,1479,227,538,685,1683,5,1761,365,384,871,1879,1983,1201,89,1420,1471,873,1521,735,352,1142,1849,1555,1636,353,497,1922,1961,1487,1231,1647,1353,1836,663,1833,1110,1371,130,1791,611,947,356,104,1924,1021,137,1759,1745,477,782,27,704,355,505,1662,769,1443,1882,1055,494,1962,18,1773,560,1392,1012,1999,8,816,235,1432,1586,1177,208,952,776,36,1152,1026,155,1257,940,1722,1751,804,372,191,136,657,1628,933,1851,1532,484,53,613,295,99,571,328,780,788,1086,1692,310,760,666,1941,312,1724,1582,348,359,297,1858,508,1154,713,546,1564,1226,853,1768,1030,1573,670,1568,169,1266,1742,1480,115,1963,1659,16,935,1216,1049,526,90,1117,846,1184,1407,141,1041,948,61,766,281,1277,1389,168,1576,143,1839,1245,965,112,1730,1990,1456,825,1898,984,290,500,1215,303,1872,1935,1294,15,379,770,1509,719,1274,334,1649,1494,803,122,93,489,1602,1518,886,1632,1921,1112,1996,1696,1950,1890,1512,1085,167,198,1359,1486,337,802,212,1560,455,619,1008,1343,1717,1784,25,969,689,1826,798,1288,826,748,262,1734,905,981,318,559,1827,1204,731,977,866,998,1998,896,1735,1280,1151,1595,705,207,1249,224,429,123,79,1539,426,1286,1788,1126,1981,380,602,211,1702,1027,1554,645,647,490,1023,1643,1862,1011,1694,204,1770,1677,37,1385,730,1500,67,747,917,1350,1705,674,922,746,109,840,1238,1672,937,1279,1687,373,1425,763,1591,265,1069,163,1214,449,664,1232,1129,75,1553,943,1801,678,100,200,1436,259,903,786,638,703,1610,1153,658,1525,834,1972,293,1604,125,80,659,1490,1741,1707,349,1721,443,195,1128,643,734,1031,525,1247,1355,124,118,68,1869,1956,574,683,1484,1130,358,1058,708,251,1514,1682,962,1220,1228,651,73,284,1877,1056,1447,762,598,1303,1071,519,1182,120,1913,1855,1794,1876,1600,1908,1401,1225,698,523,1131,1050,1603,1418,1409,797,1213,192,183,1360,1854,1748,1172,1887,612,1837,387,1344,818,1195,1365,451,503,811,1120,758,1240,1255,514,1544,1459,145,483,1047,30,308,1106,203,466,286,298,883,385,424,1125,1543,1434,1258,1679,427,1824,245,1697,1045,1900,1929,1099,1363,1579,459,849,1690,1386,361,444,906,1310,572,1945,1863,860,1789,972,580,1964,1982,405,771,47,317,1305,1919,1842,280,1412,1995,736,1476,287,1615,1488,920,1236,1969,82,721,1499,215,1460,463,214,1302,1377,1450,954,1620,1229,172,1765,907,1330,428,1635,56,1617,288,464,294,1018,671,264,1551,1404,1508,487,733,1829,39,549,1631,209,1660,755,1934,1275,690,1139,1287,819,1629,1712,1170,1168,1939,874,495,554,1542,944,1780,717,91,1038,586,551,562,325,661,1960,454,1927,1884,850,28,1300,1123,175,595,1259,697,491,470,1061,1285,561,305,1686,1651,1197,810,1155,901,20,86,1910,307,383,1022,764,921,835,1189,1075,995,1092,695,1037,1971,42,1506,241,1448,1556,1289,502,1351,1594,1208,1306,1881,741,711,1859,354,233,1191,205,1009,556,539,1792,1093,1338,465,473,1540,887,1403,790,1907,1179,263,1909,1483,1513,1703,49,1757,828,1222,131,1380,1209,1959,809,1710,945,1466,60,1039,1253,1406,1691,392,1783,1535,409,1698,946,1470,274,727,258,1968,1501,700,138,1395,1379,1132,32,1272,1357,1187,1549,452,229,416,621,955,1666,1989,12,1915,1358,1614,101,128,978,610,858,529,1113,1596,275,246,607,1060,1174,1193,1334,1766,1681,759,148,716,720,1669,1663,1337,1857,1383,1580,127,1102,1671,445,1505,552,617,1366,1203,1511,1317,1986,709,446,1265,738,1760,255,493,1904,10,1495,785,881,1787,564,590,414,474,608,13,644,1161,1270,1831,434,1233,623,1465,367,423,855,314,1290,1634,622,289,1127,1526,756,1264,301,904,1619,1327,999,291,276,440,406,1095,1054,1298,1889,193,1804,642,222,930,1068,660,1688,54,795,1375,914,1169,135,1145,1171,691,1491,1481,1227,282,346,682,1612,256,84,315,966,296,597,1079,1462,1738,1299,1457,625,1042,1726,1405,842,844,1445,1727,1367,59,1372,65,1977,888,1248,1994,116,485,750,1711,1536,1637,1814,1781,1133,1315,140,1053,808,672,58,528,230,568,1435,775,1622,1091,386,1878,1642,587,1256,807,1893,1967,351,1097,1885,1136,845,1424,1147,665,1186,330,891,113,1473,408,1762,95,371,213,1701,1162,232,1033,1763,1156,641,517,1795,692,350,898,201,1356,912,1474,248,1244,38,66,800,268,1028,1387,269,1668,1550,1010,174,1552,706,133,1799,1062,589,194,1035,1,467,1016,1871,778,1107,1314,438,518,1504,1311,347,993,1623,1336,279,223,627,506,1519,545,1776,1077,696,1368,686,509,369,565,430,544,923,1119,1007,676,1284,1199,624,626,400,76,1592,847,19,654,377,629,583,1224,1948,401,1267,1438,105,316,1084,1510,1891,1124,1695,345,1316,31,1601,553,1180,684,1866,1439,524,1493,591,997,1100,319,749,72,1108,1065,1354,1024,1593,403,1650,1670,253,1104,628,218,149,729,1803,270,364,171,1261,829,967,1427,767,238,1965,1417,635,1135,1984,247,1903,963,1175,202,879,1820,859,814,899,458,1325,1828,1812,368,1250,783,417,1361,1429,1930,1109,1985,1785,744,1352,982,1242,23,1319,14,521,1254,1800,1993,1422,1588,1149,64,77,1064,1402,1002,603,656,260,1589,801,111,1433,1188,2,932,787,894,278,1830,817,878,496,1003,1461,926,1988,707,1269,419,722,1706,164,1292,1446,1394,1260,402,1940,1731,1297,1320,1321,326,33,956,357,1606,1665,1626,1192,271,824,1408,1911,1032,1875,1000,237,936,395,872,1868,1609,1243,110,1468,1864,851,1296,40,378,46,1991,652,1115,336,875,34,718,50,1437,404,478,1667,1754,1577,1902,190,250,1957,157,1607,422,283,1339,1654,488,1546,913,1341,1633,363,1714,341,1821,179,1072,862,880,1067,919,1708,462,1101,1307,1822,1282,1786,1006,17,1531,1925,827,442,1570,1211,1775,1652,1423,425,360,1415,1725,1527,1239,994,1987,1044,1309,1458,181,153,604,1207,1709,599,1797,1737,1398,1212,1414,390,653,1958,1569,173,1390,1772,675,751,1173,1638,1001,1273,1017,1472,1523,1440,139,1943,197,1497,1202,1313,106,1888,631,331,830,1442,119,681,479,1992,393,582,1397,724,333,431,618,1538,216,714,1217,1318,634,1503,1087,1802,156,85,1646,415,1598,680,757,469,530,221,1362,754,1234,1122,712,1810,302,1276,1874,1475,1116,1013,1860,1308,1916,1973,411,1736,893,178,1074,1823,391,768,1452,261,1463,677,1769,7,1205,852,989,961,1684,1566,126,1455,6,45,69,925,1376,226,534,1143,285,1933,1416,520,1103,931,338,1850,219,180,1818,1942,725,1057,861,1931,102,1756,335,162,362,1778,188,960,1583,254,87,1928,884,1482,532,1043,447,1923,1160,1561,1618,1335,457,805,1347,132,975,1345,916,435,516,813,311,1571,1263,941,1137,1834,9,394,182,1237,1790,1015,512,1410,958,726,161,823,510,1567,468,1369,252,52,1166,1159,381,988,1955,499,35,1293,1348,1426,1946,1393,329,1478,563,501,1477,1485,779,239,1895,1070,581,1847,117,92,1740,146,243,789,1378,992,951,98,142,121,1190,1575,306,48,636,1675,1680,1661,129,170,938,1052,986,964,620,1899,166,159,870,1616,815,1066,1322,662,1349,481,1219,1816,475,1777,396,1597,976,57,71,1723,4,186,1134,555,1105,791,1088,321,507,165,1520,1496,1076,885,1920,536,918,991,1747,1753,1373,1811,1852,1693,1111,796,1449,1324,1251,1382,1331,793,320,277,1183,1563,1979,1767,1530,
]

def read_graph(idx):
    filename = "datasets/train/{}_graph.txt".format(SHUFFLED_INDEX[idx])
    return np.loadtxt(filename, skiprows=1)

def read_label(idx):
    filename = "datasets/train/{}_label.txt".format(SHUFFLED_INDEX[idx])
    f = open(filename, "r")
    return int(f.read())

def read_train_data():
    return np.array([(read_graph(i), read_label(i)) for i in range(0, TRAIN_NUM)])

def read_test_data():
    return np.array([(read_graph(i), read_label(i)) for i in range(TRAIN_NUM, TRAIN_NUM + TEST_NUM)])

# (loss, accuracy)の平均を計算
def test(gnn, data):
    loss_mean = np.mean([gnn.loss(g, l) for g, l in data])
    accuracy_mean = np.mean([gnn.predict(g) == l for g, l in data])
    return loss_mean, accuracy_mean

def train(gnn, train_data, test_data, epoch_num=100, print_train_loss=False):
    for epoch in range(epoch_num):
        np.random.shuffle(train_data)
        iter_num = TRAIN_NUM // MINIBATCH_SIZE
        for mb_idx in range(iter_num):
            minibatch = train_data[mb_idx * MINIBATCH_SIZE : (mb_idx + 1) * MINIBATCH_SIZE]
            gnn.gradient_descent(minibatch)
            # 1 epoch中に2回testを計算
            if mb_idx in [0, iter_num // 2]:
                print("test: {}".format(test(gnn, test_data)))
                if print_train_loss:
                    print("train: {}".format(test(gnn, train_data)))

if __name__ == "__main__":
    train_data = read_train_data()
    test_data = read_test_data()

    for i in range(5):
        print("{}th model".format(i))
        gnn = GraphNeuralNetwork(Adam())
        train(gnn, train_data, test_data, epoch_num=100)
        path_name = "model/model{}.pickle".format(i)
        os.makedirs(os.path.dirname(path_name), exist_ok=True)
        with open(path_name, mode="wb") as f:
            pickle.dump(gnn, f)
        print("{} saved!".format(path_name))